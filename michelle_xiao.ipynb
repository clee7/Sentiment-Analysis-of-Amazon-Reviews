{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Jenny branch\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, BatchNormalization\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, skiprows = 1):\n",
    "    \"\"\"\n",
    "    Function loads data stored in the file filename and returns it as a numpy ndarray.\n",
    "    \n",
    "    Inputs:\n",
    "        filename: given as a string.\n",
    "        \n",
    "    Outputs:\n",
    "        Data contained in the file, returned as a numpy ndarray\n",
    "    \"\"\"\n",
    "    return np.loadtxt(filename, skiprows=skiprows, delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads training dataset\n",
    "training = load_data('data/training_data.txt', 1)\n",
    "X_train = training[:, 1:]\n",
    "Y_train = training[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads testing dataset\n",
    "# There is no label for testing set \n",
    "X_test = load_data('data/test_data.txt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Y values to one hot vector\n",
    "num_classes = 2\n",
    "Y_train = keras.utils.to_categorical(Y_train, num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input size\n",
    "n_train = X_train.shape[0]\n",
    "n_words = X_train.shape[1]\n",
    "n_test = Y_train.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 1D Convolution, change dimension of input \n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 1000, 5)           20        \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1000, 5)           20        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1000, 5)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000, 5)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1000, 5)           55        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1000, 5)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 500, 5)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500, 5)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2500)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               250100    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 202       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 250,397\n",
      "Trainable params: 250,387\n",
      "Non-trainable params: 10\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "## Create your own model here given the constraints in the problem\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(5, 3, padding=\"same\", input_shape=(n_words, 1, )))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv1D(5, 2, padding=\"same\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "## Once you one-hot encode the data labels, the line below should be predicting probabilities of each of the 2 classes\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "## Printing a summary of the layers and weights in your model\n",
    "model.summary()\n",
    "\n",
    "## In the line below we have specified the loss function as 'mse' (Mean Squared Error) because in the above code we did not one-hot encode the labels.\n",
    "## In your implementation, since you are one-hot encoding the labels, you should use 'categorical_crossentropy' as your loss.\n",
    "## You will likely have the best results with RMS prop or Adam as your optimizer.  In the line below we use Adadelta\n",
    "model.compile(loss='categorical_crossentropy',optimizer='RMSprop', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 13s 815us/step - loss: 0.0868 - acc: 0.9706\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 14s 855us/step - loss: 0.0731 - acc: 0.9738\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 13s 785us/step - loss: 0.0670 - acc: 0.9776\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 13s 802us/step - loss: 0.0627 - acc: 0.9792\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 12s 744us/step - loss: 0.0557 - acc: 0.9813\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 12s 749us/step - loss: 0.0544 - acc: 0.9829\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 12s 749us/step - loss: 0.0466 - acc: 0.9842\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 13s 806us/step - loss: 0.0480 - acc: 0.9846\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 13s 812us/step - loss: 0.0468 - acc: 0.9843\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 12s 754us/step - loss: 0.0424 - acc: 0.9871\n",
      "16000/16000 [==============================] - 5s 317us/step\n",
      "4000/4000 [==============================] - 1s 325us/step\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 12s 769us/step - loss: 0.2446 - acc: 0.9406\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 12s 747us/step - loss: 0.1470 - acc: 0.9553\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 12s 755us/step - loss: 0.1136 - acc: 0.9629\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 12s 764us/step - loss: 0.0937 - acc: 0.9691\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 12s 750us/step - loss: 0.0752 - acc: 0.9748\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 12s 739us/step - loss: 0.0619 - acc: 0.9804\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 14s 890us/step - loss: 0.0631 - acc: 0.9808\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 13s 791us/step - loss: 0.0615 - acc: 0.9807\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 12s 751us/step - loss: 0.0483 - acc: 0.9848\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 12s 754us/step - loss: 0.0447 - acc: 0.9864\n",
      "16000/16000 [==============================] - 5s 316us/step\n",
      "4000/4000 [==============================] - 1s 317us/step\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 12s 766us/step - loss: 0.0800 - acc: 0.9749\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 12s 738us/step - loss: 0.0577 - acc: 0.9825\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 12s 773us/step - loss: 0.0561 - acc: 0.9827\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 12s 741us/step - loss: 0.0463 - acc: 0.9860\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 12s 767us/step - loss: 0.0468 - acc: 0.9874\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 12s 738us/step - loss: 0.0367 - acc: 0.9880\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 12s 736us/step - loss: 0.0404 - acc: 0.9869\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 12s 740us/step - loss: 0.0324 - acc: 0.9905\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 12s 723us/step - loss: 0.0375 - acc: 0.9889\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 12s 744us/step - loss: 0.0387 - acc: 0.9889\n",
      "16000/16000 [==============================] - 5s 324us/step\n",
      "4000/4000 [==============================] - 1s 328us/step\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 12s 732us/step - loss: 0.0566 - acc: 0.9827\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 13s 789us/step - loss: 0.0438 - acc: 0.9868\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 13s 813us/step - loss: 0.0417 - acc: 0.9882\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 13s 792us/step - loss: 0.0375 - acc: 0.9892\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 12s 753us/step - loss: 0.0386 - acc: 0.9881\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 12s 752us/step - loss: 0.0334 - acc: 0.9900\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 13s 786us/step - loss: 0.0353 - acc: 0.9898\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 12s 770us/step - loss: 0.0354 - acc: 0.9901\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 12s 738us/step - loss: 0.0302 - acc: 0.9906\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 12s 741us/step - loss: 0.0284 - acc: 0.9921\n",
      "16000/16000 [==============================] - 5s 319us/step\n",
      "4000/4000 [==============================] - 1s 331us/step\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 13s 842us/step - loss: 0.0451 - acc: 0.9871\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 13s 800us/step - loss: 0.0363 - acc: 0.9900\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 13s 782us/step - loss: 0.0287 - acc: 0.9917\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 13s 843us/step - loss: 0.0325 - acc: 0.9911\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 14s 847us/step - loss: 0.0306 - acc: 0.9914\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 12s 754us/step - loss: 0.0335 - acc: 0.9910\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 12s 747us/step - loss: 0.0287 - acc: 0.9910\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 14s 860us/step - loss: 0.0279 - acc: 0.9924\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 12s 762us/step - loss: 0.0248 - acc: 0.9931\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 13s 793us/step - loss: 0.0270 - acc: 0.9931\n",
      " 5248/16000 [========>.....................] - ETA: 3s"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 5)\n",
    "inds = [ind for ind in kf.split(X_train, Y_train)]\n",
    "        \n",
    "total_train_acc = 0\n",
    "total_val_acc = 0\n",
    "    \n",
    "# perform 5-fold validation\n",
    "for i in range(0,5):\n",
    "    traini, vali = inds[i]\n",
    "    fit = model.fit(X_train[traini], Y_train[traini], batch_size=128, epochs=10, verbose=1)\n",
    "            \n",
    "    # Compute accuracy.\n",
    "    train_acc = model.evaluate(X_train[traini], Y_train[traini], verbose=1)\n",
    "    val_acc = model.evaluate(X_train[vali], Y_train[vali], verbose=1)\n",
    "            \n",
    "    total_train_acc += train_acc[1]\n",
    "    total_val_acc += val_acc[1]\n",
    "        \n",
    "print(\"average training accuracy\", total_train_acc / 5.)\n",
    "print(\"average val accuracy\", total_val_acc / 5.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing the accuracy of our model, according to the loss function specified in model.compile above\n",
    "score = model.evaluate(X_train, Y_train, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test, verbose=1)\n",
    "print(prediction)\n",
    "\n",
    "zeros = prediction[:, 0]\n",
    "ones  = prediction[:, 1]\n",
    "\n",
    "prediction = (zeros < ones).astype(int)\n",
    "\n",
    "print(prediction)\n",
    "print(\"prediction shape is: {}\".format(prediction.shape))\n",
    "\n",
    "output = \"Id,Prediction\\n\"\n",
    "for i in range(prediction.shape[0]):\n",
    "    output = output + (\"{0},{1}\\n\".format(i + 1, prediction[i]))\n",
    "\n",
    "file = open('predictions.csv','w') \n",
    "file.write(output)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
